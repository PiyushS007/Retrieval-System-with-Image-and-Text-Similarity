{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image,ImageEnhance\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pandas as pd\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "\n",
        "def adjust_contrast_brightness(img, contrast_factor=1.2, brightness_factor=1.2):\n",
        "\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img = enhancer.enhance(contrast_factor)\n",
        "\n",
        "    enhancer = ImageEnhance.Brightness(img)\n",
        "    img = enhancer.enhance(brightness_factor)\n",
        "\n",
        "    return img\n",
        "\n",
        "def load_image_from_url(url, target_size=(224, 224), contrast_factor=1.2, brightness_factor=1.2):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "\n",
        "        # Adjust contrast and brightness\n",
        "        img = adjust_contrast_brightness(img, contrast_factor, brightness_factor)\n",
        "\n",
        "        img = img.resize(target_size)\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = preprocess_input(img)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image from URL {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_features_from_urls(image_urls):\n",
        "    features_list = []\n",
        "\n",
        "    for urls in image_urls:\n",
        "        best_feature = None\n",
        "        best_score = float('-inf')\n",
        "\n",
        "        for url in urls:  # Assuming URLs are stored as a string representation of a list\n",
        "            img = load_image_from_url(url)\n",
        "            img_features = model.predict(img)\n",
        "\n",
        "            # Compare current feature score with the best score\n",
        "            if np.sum(img_features) > best_score:\n",
        "                best_score = np.sum(img_features)\n",
        "                best_feature = img_features\n",
        "\n",
        "        features_list.append(best_feature)\n",
        "\n",
        "    return features_list\n",
        "\n",
        "\n",
        "\n",
        "model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "    # Read CSV file containing image URLs and text reviews\n",
        "df = pd.read_csv(\"A2_Data.csv\")\n",
        "\n",
        "# Extract image URLs from the 'image' column\n",
        "image_urls_list = df['Image'].apply(eval).tolist()\n",
        "\n",
        "# Extract features from images\n",
        "extracted_features = extract_features_from_urls(image_urls_list)\n",
        "\n",
        "# Convert the list of feature vectors into a numpy array\n",
        "feature_matrix = np.array(extracted_features)\n",
        "\n",
        "# Normalize the feature matrix\n",
        "normalized_features = (feature_matrix - np.mean(feature_matrix)) / np.std(feature_matrix)\n"
      ],
      "metadata": {
        "id": "1dJsHhzwCmo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_features_list = []\n",
        "\n",
        "# Iterate through each sublist in normalized_features_list\n",
        "for sublist in normalized_features:\n",
        "    # Iterate through each element in the sublist\n",
        "    for item in sublist:\n",
        "        # Append the element to the single list\n",
        "        normalized_features_list.append(item)\n",
        "\n",
        "print(normalized_features_list[993])"
      ],
      "metadata": {
        "id": "5lqNuBEbJl3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Store as pickle\n",
        "with open('normalized_features_list_latest.pkl', 'wb') as f:\n",
        "    pickle.dump(normalized_features_list, f)\n",
        "\n",
        "# Load the pickle file\n",
        "with open('normalized_features_list_latest.pkl', 'rb') as f:\n",
        "    loaded_normalized_features_list = pickle.load(f)"
      ],
      "metadata": {
        "id": "9iq3xO7RKP12"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new_tst= pd.DataFrame({'vector_column': loaded_normalized_features_list})\n",
        "\n",
        "\n",
        "df_latest = pd.read_csv(\"A2_Data.csv\")\n",
        "\n",
        "concatenated_df = pd.concat([df_latest, df_new_tst], axis=1)\n",
        "\n",
        "# Reset index of the concatenated DataFrame\n",
        "concatenated_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "concatenated_df"
      ],
      "metadata": {
        "id": "ZM21I4zLK2vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import string\n",
        "import math\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "custom_punctuation = string.punctuation + '“”‘’'\n",
        "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocessing_text_mod(text):\n",
        "    if pd.isna(text):\n",
        "        return ''  # Return an empty string for NaN values\n",
        "    text = str(text)  # Convert to string to handle non-string values\n",
        "    text = text.lower()  # Lowercase the text\n",
        "    tokens = tokenizer.tokenize(text)  # Tokenization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [token for token in tokens if token not in custom_punctuation]  # Remove punctuations\n",
        "    tokens = [token for token in tokens if token.strip() != '']\n",
        "\n",
        "\n",
        "    # Stemming\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess the content\n",
        "concatenated_df['Preprocessed_Content'] = concatenated_df['Review Text'].apply(preprocessing_text_mod)\n",
        "\n",
        "# Calculate TF-IDF scores\n",
        "def calculate_tf(word_list):\n",
        "    word_tf = {}\n",
        "    total_words = len(word_list)\n",
        "    for word in word_list:\n",
        "        word_tf[word] = word_tf.get(word, 0) + 1 / total_words\n",
        "    return word_tf\n",
        "\n",
        "def calculate_idf(documents):\n",
        "    word_idf = {}\n",
        "    total_documents = len(documents)\n",
        "    all_words = set([word for sublist in documents for word in sublist])\n",
        "    for word in all_words:\n",
        "        count = sum([1 for document in documents if word in document])\n",
        "        word_idf[word] = math.log(total_documents / count)\n",
        "    return word_idf\n",
        "\n",
        "def calculate_tf_idf(tf, idf):\n",
        "    tf_idf = {}\n",
        "    for word, tf_value in tf.items():\n",
        "        tf_idf[word] = tf_value * idf[word]\n",
        "    return tf_idf\n",
        "\n",
        "documents = [word_tokenize(doc) for doc in concatenated_df['Preprocessed_Content']]\n",
        "\n",
        "word_tf_list = [calculate_tf(doc) for doc in documents]\n",
        "\n",
        "word_idf = calculate_idf(documents)\n",
        "\n",
        "tf_idf_list = [calculate_tf_idf(tf, word_idf) for tf in word_tf_list]\n",
        "\n",
        "\n",
        "\n",
        "#Convert tf-idf to DataFrame\n",
        "#tf_idf_df = pd.DataFrame(tf_idf_list)\n",
        "#df1 = pd.concat([df, tf_idf_df], axis=1)\n",
        "\n",
        "#df1\n"
      ],
      "metadata": {
        "id": "zlU3R3d1LWEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def cosine_similarity_fresh(vec1, vec2):\n",
        "\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 != 0 and norm_vec2 != 0:\n",
        "        similarity_fresh = dot_product / (norm_vec1 * norm_vec2)\n",
        "    else:\n",
        "        similarity_fresh = 0  # If any of the vectors is a zero vector, cosine similarity is 0.\n",
        "\n",
        "    return similarity_fresh\n",
        "\n",
        "\n",
        "# Calculate TF-IDF scores for the input text review\n",
        "def calculate_similarity(input_review):\n",
        "    preprocessed_input_review = preprocessing_text_mod(input_review)\n",
        "    input_review_tokens = word_tokenize(preprocessed_input_review)\n",
        "    input_review_tf = calculate_tf(input_review_tokens)\n",
        "    input_review_tf_idf = calculate_tf_idf(input_review_tf, word_idf)\n",
        "\n",
        "    # Represent the input text review as a vector\n",
        "    input_review_vector = [input_review_tf_idf.get(word, 0) for word in word_idf.keys()]\n",
        "\n",
        "    # Compute cosine similarity between input review vector and each review vector in DataFrame\n",
        "    similarities1 = []\n",
        "    for tf_idf in tf_idf_list:\n",
        "        review_vector = [tf_idf.get(word, 0) for word in word_idf.keys()]\n",
        "        similarity = cosine_similarity_fresh(input_review_vector, review_vector)\n",
        "        similarities1.append(similarity)\n",
        "\n",
        "    return similarities1\n",
        "\n",
        "# Add similarity scores to DataFrame\n",
        "\n",
        "\n",
        "\n",
        "def takingReview(review_input):\n",
        "\n",
        "  similarities = calculate_similarity(review_input)\n",
        "  concatenated_df['Cosine_Similarity_text'] = similarities\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "su8hNopWLgCD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cosine_similarity(input_features, target_features):\n",
        "    similarities = cosine_similarity(input_features, target_features)\n",
        "    return similarities.flatten()\n",
        "\n",
        "\n",
        "def calculate_image_similarity(user_url):\n",
        "    user_features = load_image_from_url(user_url)\n",
        "    if user_features is not None:\n",
        "        user_features = model.predict(user_features)\n",
        "        user_normalized_features = (user_features - np.mean(user_features)) / np.std(user_features)\n",
        "\n",
        "        # Calculate cosine similarities with each image in the DataFrame column\n",
        "        similarities_list1 = []\n",
        "        for features in concatenated_df['vector_column']:\n",
        "            similarity = calculate_cosine_similarity(user_normalized_features, np.array(features).reshape(1, -1))\n",
        "            similarities_list1.append(similarity[0])\n",
        "\n",
        "        return similarities_list1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def takingUrl(url_input):\n",
        "\n",
        "  similarities_list = calculate_image_similarity(url_input)\n",
        "\n",
        "  concatenated_df['Cosine_Similarity_images'] = similarities_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KZRi2MdPLn3c"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_txt=input(\"Enter input review\")#\"guitar is very\"\n",
        "input_img_url=input(\"enter your input url\")#\"https://images-na.ssl-images-amazon.com/images/I/71Isri9SEaL._SY88.jpg\""
      ],
      "metadata": {
        "id": "wOh085EaOFjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "takingReview(input_txt)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "takingUrl(input_img_url)"
      ],
      "metadata": {
        "id": "kA72vf31MQVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_df['average'] = (concatenated_df['Cosine_Similarity_text'] + concatenated_df['Cosine_Similarity_images']) / 2\n",
        "concatenated_df"
      ],
      "metadata": {
        "id": "Ey4JzCNCMf1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = concatenated_df.sort_values(by='Cosine_Similarity_images', ascending=False)\n",
        "\n",
        "result_df"
      ],
      "metadata": {
        "id": "E3LlFBDoMqbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted_on_text = concatenated_df.sort_values(by='Cosine_Similarity_text', ascending=False)\n",
        "df_sorted_on_text"
      ],
      "metadata": {
        "id": "6MyTlHLjMzrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataframe_info(dataframe):\n",
        "    for index, row in dataframe.head(3).iterrows():\n",
        "        print(\"Image URL:\", row['Image'])\n",
        "        print(\"Review:\", row['Review Text'])\n",
        "        print(\"Cosine similarity of images -\", '{:.4f}'.format(row['Cosine_Similarity_images']))\n",
        "        print(\"Cosine similarity of text -\", '{:.3f}'.format(row['Cosine_Similarity_text']))\n",
        "        print(\"Composite score -\", '{:.3f}'.format(row['average']))\n",
        "        print()\n",
        "    # subset1 = dataframe['Cosine_Similarity_text'].head(3)\n",
        "    # # Calculate the average of the subset\n",
        "    # avg1 = subset1.mean()\n",
        "    # subset2 = dataframe['Cosine_Similarity_images'].head(3)\n",
        "    # # Calculate the average of the subset\n",
        "    # avg2 = subset2.mean()\n",
        "    # total_avg=(avg1+avg2)/2\n",
        "\n",
        "    # print(\"Composite similarity scores of images: {:.4f}\".format(avg2))\n",
        "    # print(\"Composite similarity scores of text: {:.4f}\".format(avg1))\n",
        "    # print(\"Final composite similarity score: {:.4f}\".format(total_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Assuming `df` is your DataFrame with columns 'Image URL', 'Review', 'Cosine similarity of images', 'Cosine similarity of text'\n",
        "print(\"Image based retrival\")\n",
        "print_dataframe_info(result_df)\n",
        "\n",
        "print(\"Text based retrival\")\n",
        "print_dataframe_info(df_sorted_on_text)"
      ],
      "metadata": {
        "id": "wNjFEj4QM6Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted_on_composite=concatenated_df.sort_values(by='average', ascending=False)\n",
        "df_sorted_on_composite"
      ],
      "metadata": {
        "id": "2tiXInnuNquJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"composite score based retrival\")\n",
        "print_dataframe_info(df_sorted_on_composite)"
      ],
      "metadata": {
        "id": "pxDl4qJkNtTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}